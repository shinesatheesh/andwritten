{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7a723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90277e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (20, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2617351",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 28\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m      6\u001b[0m x_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m----> 7\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 28\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "x_train = cv2.cvtColor(x_train,cv2.COLOR_BGR2GRAY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4ee0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-28 21:18:00.989036: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2562 - accuracy: 0.9265\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1129 - accuracy: 0.9668\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0773 - accuracy: 0.9765\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0583 - accuracy: 0.9819\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0455 - accuracy: 0.9862\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0344 - accuracy: 0.9895\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0279 - accuracy: 0.9912\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0230 - accuracy: 0.9931\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0144 - accuracy: 0.9960\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0113 - accuracy: 0.9966\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0104 - accuracy: 0.9969\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - accuracy: 0.9971\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0062 - accuracy: 0.9982\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0058 - accuracy: 0.9982\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0041 - accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0072 - accuracy: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2948da1820>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape =(28,28)),\n",
    "    keras.layers.Dense(128,activation=\"relu\"),\n",
    "    keras.layers.Dense(10,activation = \"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(x_train,y_train,epochs =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c74030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: recognition/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92764b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28635eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index =12\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())\n",
    "\n",
    "\n",
    "def makeSquare(not_square):\n",
    "    \n",
    "    BLACK = [0,0,0]\n",
    "    img_dim = not_square.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "    \n",
    "    if (height == width):\n",
    "        square = not_square\n",
    "        return square\n",
    "    else:\n",
    "        doublesize = cv2.resize(not_square,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "        height = height * 2\n",
    "        width = width * 2\n",
    "        \n",
    "        if (height > width):\n",
    "            pad = int((height - width)/2)\n",
    "            \n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,0,0,pad,\\\n",
    "                                                   pad,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "        else:\n",
    "            pad = int((width - height)/2)\n",
    "            \n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,pad,pad,0,0,\\\n",
    "                                                   cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    doublesize_square_dim = doublesize_square.shape\n",
    "    \n",
    "    return doublesize_square\n",
    "\n",
    "\n",
    "def resize_to_pixel(dimensions, image):\n",
    "    \n",
    "    \n",
    "    buffer_pix = 4\n",
    "    dimensions  = dimensions - buffer_pix\n",
    "    squared = image\n",
    "    r = float(dimensions) / squared.shape[1]\n",
    "    dim = (dimensions, int(squared.shape[0] * r))\n",
    "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    img_dim2 = resized.shape\n",
    "    height_r = img_dim2[0]\n",
    "    width_r = img_dim2[1]\n",
    "    BLACK = [0,0,0]\n",
    "    if (height_r > width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,0,0,0,1,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    if (height_r < width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,1,0,0,0,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    p = 2\n",
    "    ReSizedImg = cv2.copyMakeBorder(resized,p,p,p,p,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    img_dim = ReSizedImg.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "    \n",
    "    return ReSizedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52249b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./image/1.jpg')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "# Blur image then find \n",
    "#edges using Canny \n",
    "blurred = cv2.GaussianBlur(gray, (5,5),0)\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "plt.imshow(edged)\n",
    "contours, _ = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "full_number = []\n",
    "for c in contours:\n",
    "    # compute the bounding box for the rectangle\n",
    "    (x, y, w, h) = cv2.boundingRect(c)    \n",
    "    \n",
    "    #cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "    #cv2.imshow(\"Contours\", image)\n",
    "\n",
    "    if w >=5  and h >=25:\n",
    "        roi = blurred[y:y + h, x:x + w]\n",
    "        ret, roi = cv2.threshold(roi, 127, 255,cv2.THRESH_BINARY_INV)\n",
    "        squared = makeSquare(roi)\n",
    "        final = resize_to_pixel(20, squared)\n",
    "        final_array = final.reshape((1,400))\n",
    "        final_array = final_array.astype(np.float32)\n",
    "     \n",
    "        cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in preprocessed_digits:    prediction = model.predict(final.reshape(1, 28, 28, 1))  \n",
    "    print (\"=========PREDICTION============ \\n\\n\")\n",
    "    plt.imshow(digit.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\"\\n\\nFinal Output: {}\".format(np.argmax(prediction)))\n",
    "    \n",
    "    print (\"\\nPrediction (Softmax) from the neural network:\\n\\n {}\".format(prediction))\n",
    "        hard_maxed_prediction = np.zeros(prediction.shape)\n",
    "    hard_maxed_prediction[0][np.argmax(prediction)] = 1\n",
    "    print (\"\\n\\nHard-maxed form of the prediction: \\n\\n {}\".format(hard_maxed_prediction))    print (\"\\n\\n---------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744ff54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
